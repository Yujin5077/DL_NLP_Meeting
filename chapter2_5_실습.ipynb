{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd6f6f3",
   "metadata": {},
   "source": [
    "#### 영어 n-gram 구현\n",
    "    - 데이터는 직접 작성한 문장을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d6531bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if n = 3, I like 를 입력하면 pizza 가 나와야함 \n",
    "data = [\"I like pizza but you hate pizza\",\n",
    "        \"I like pizza We will go to eat pizza\",\n",
    "        \"I like chiken you like pizza\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972e3d9e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "토큰화한 첫 문장: ['i', 'like', 'pizza', 'but', 'you', 'hate', 'pizza']\n"
     ]
    }
   ],
   "source": [
    "# 각각의 문서를 토크나이즈 함\n",
    "%time\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "doc_list = []\n",
    "for idx in range(len(data)):\n",
    "    doc_ = data[idx]\n",
    "    # 알파벳을 제외하고 ' '으로 변경\n",
    "    doc_ = re.sub('[^a-zA-Z]', ' ', doc_)\n",
    "    # 소문자로 전환\n",
    "    doc_ = doc_.lower()\n",
    "    \n",
    "    token = word_tokenize(doc_)\n",
    "    doc_list.append(token)\n",
    "    \n",
    "print(\"토큰화한 첫 문장:\", doc_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b77a756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global n_minus_count, n_all_count, n_gram, n\n",
    "\n",
    "n = 3\n",
    "\n",
    "# n-1 단어 카운트\n",
    "n_minus_count = {}\n",
    "\n",
    "# n 단어 카운트 \n",
    "n_all_count = {}\n",
    "\n",
    "# 단어 뭉치\n",
    "n_gram = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9d689b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 뭉치 제작\n",
    "n_gram = []\n",
    "for sent in doc_list:\n",
    "    for idx in reversed(range(len(sent))):\n",
    "        if idx >= n - 1:\n",
    "            n_gram_li = sent[idx-n+1:idx+1]\n",
    "            n_gram.append(n_gram_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a982a6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram 단어뭉치: [['you', 'hate', 'pizza'], ['but', 'you', 'hate'], ['pizza', 'but', 'you'], ['like', 'pizza', 'but'], ['i', 'like', 'pizza'], ['to', 'eat', 'pizza'], ['go', 'to', 'eat'], ['will', 'go', 'to'], ['we', 'will', 'go'], ['pizza', 'we', 'will'], ['like', 'pizza', 'we'], ['i', 'like', 'pizza'], ['you', 'like', 'pizza'], ['chiken', 'you', 'like'], ['like', 'chiken', 'you'], ['i', 'like', 'chiken']]\n",
      "단어 뭉치 개수: [['you', 'hate', 'pizza'], ['but', 'you', 'hate'], ['pizza', 'but', 'you'], ['like', 'pizza', 'but'], ['i', 'like', 'pizza'], ['to', 'eat', 'pizza'], ['go', 'to', 'eat'], ['will', 'go', 'to'], ['we', 'will', 'go'], ['pizza', 'we', 'will'], ['like', 'pizza', 'we'], ['i', 'like', 'pizza'], ['you', 'like', 'pizza'], ['chiken', 'you', 'like'], ['like', 'chiken', 'you'], ['i', 'like', 'chiken']]\n"
     ]
    }
   ],
   "source": [
    "print(\"n-gram 단어뭉치:\", n_gram)\n",
    "print(\"단어 뭉치 개수:\", n_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffde4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def n_minus(corpus):\n",
    "    minus_all = []\n",
    "    for i in n_gram:\n",
    "        sent = \"\"\n",
    "        for j in range(n-1):\n",
    "            if j == 0:\n",
    "                sent = i[j]\n",
    "            else:\n",
    "                sent = sent + \" \" + i[j]\n",
    "        minus_all.append(sent)\n",
    "    count = Counter(minus_all)\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f410a365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'you hate': 1,\n",
       "         'but you': 1,\n",
       "         'pizza but': 1,\n",
       "         'like pizza': 2,\n",
       "         'i like': 3,\n",
       "         'to eat': 1,\n",
       "         'go to': 1,\n",
       "         'will go': 1,\n",
       "         'we will': 1,\n",
       "         'pizza we': 1,\n",
       "         'you like': 1,\n",
       "         'chiken you': 1,\n",
       "         'like chiken': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_minus_count = n_minus(n_gram)\n",
    "n_minus_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb600f4",
   "metadata": {},
   "source": [
    "    - n_minus_count는 n-1의 동일한 단어뭉치들의 수를 카운트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe4dc551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_all(corpus):\n",
    "    n_all = {}\n",
    "    for i in n_gram:\n",
    "        sent = []\n",
    "        for j in range(n):\n",
    "            if j == 0:\n",
    "                sent = i[j]\n",
    "            elif j < n-1:\n",
    "                sent = sent + \" \" + i[j]\n",
    "            else:\n",
    "                if sent not in n_all.keys():\n",
    "                    n_all[sent] = [i[j]]\n",
    "                else:\n",
    "                    n_all[sent].append(i[j])\n",
    "    \n",
    "    return n_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1fea330",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'you hate': ['pizza'],\n",
       " 'but you': ['hate'],\n",
       " 'pizza but': ['you'],\n",
       " 'like pizza': ['but', 'we'],\n",
       " 'i like': ['pizza', 'pizza', 'chiken'],\n",
       " 'to eat': ['pizza'],\n",
       " 'go to': ['eat'],\n",
       " 'will go': ['to'],\n",
       " 'we will': ['go'],\n",
       " 'pizza we': ['will'],\n",
       " 'you like': ['pizza'],\n",
       " 'chiken you': ['like'],\n",
       " 'like chiken': ['you']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_all_count = n_all(n_gram)\n",
    "n_all_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcd5ff",
   "metadata": {},
   "source": [
    "    - n-1개의 동일한 단어뭉치의 뒷 단어로 어떤 단어들이 존재할 수 있는지 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5d6dc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-1 단어뭉치 카운트: Counter({'i like': 3, 'like pizza': 2, 'you hate': 1, 'but you': 1, 'pizza but': 1, 'to eat': 1, 'go to': 1, 'will go': 1, 'we will': 1, 'pizza we': 1, 'you like': 1, 'chiken you': 1, 'like chiken': 1})\n",
      "n 단어뭉치 카운트: {'you hate': ['pizza'], 'but you': ['hate'], 'pizza but': ['you'], 'like pizza': ['but', 'we'], 'i like': ['pizza', 'pizza', 'chiken'], 'to eat': ['pizza'], 'go to': ['eat'], 'will go': ['to'], 'we will': ['go'], 'pizza we': ['will'], 'you like': ['pizza'], 'chiken you': ['like'], 'like chiken': ['you']}\n"
     ]
    }
   ],
   "source": [
    "print(\"n-1 단어뭉치 카운트:\", n_minus_count)\n",
    "print(\"n 단어뭉치 카운트:\", n_all_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e5f72ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'pizza': 2, 'chiken': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(n_all_count[\"i like\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c6d560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_minus_count[\"i like\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7df071bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_gram(front_sent):\n",
    "    front_sent = front_sent.lower()\n",
    "    if front_sent in n_minus_count.keys():\n",
    "        minus_count = n_minus_count[front_sent]\n",
    "        # n-1 앞에 존재하는 단어뭉치 중 원하는 결과값을 찾아 해당 단어뒤에 존재하는 단어를 카운트 해준다\n",
    "        all_count = Counter(n_all_count[front_sent])\n",
    "        # 카운트 해준 단어 중 가장 많이 등장한 단어 나오도록 정렬해준다.\n",
    "        all_count = sorted(all_count.items(), key=lambda items: items[1], reverse=True)\n",
    "        \n",
    "        # 가장 많이 나온 단어\n",
    "        max_word = all_count[0][0]\n",
    "        # 가장 많이 나온 단어가 등장할 확률\n",
    "        per_ = (all_count[0][1])/(minus_count)\n",
    "        return print(max_word,\"가 나올 확률이 {0:0.2f}\".format(per_), \"%로 가장 높습니다.\")\n",
    "    else:\n",
    "        return print(\"해당 단어가 존재하지 않습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea83a5e8",
   "metadata": {},
   "source": [
    "    - 앞의 단어를 입력하였을 때 뒤의 단어에 어떤 단어가 존재 할 수 있는지 알려준다. \n",
    "    - 만약 앞의 n - 1의 단어가 존재하지 않는다면 존재하지 않는다고 출력해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "814c2d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza 가 나올 확률이 0.67 %로 가장 높습니다.\n"
     ]
    }
   ],
   "source": [
    "n_gram(\"I like\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fcc876b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 단어가 존재하지 않습니다.\n"
     ]
    }
   ],
   "source": [
    "n_gram(\"I love\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1350e9ae",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bfa4d2",
   "metadata": {},
   "source": [
    "#### 영어 BoW 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58d3edda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 불용어 개수: 179\n",
      "영어 불용어 10개 출력: ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word_list = stopwords.words(\"english\")\n",
    "\n",
    "print(\"영어 불용어 개수:\", len(stop_word_list))\n",
    "print(\"영어 불용어 10개 출력:\", stop_word_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82052372",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Family is not an important thing. it's everything.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f41fa61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def en_bag_of_words(doc):\n",
    "    doc = re.sub(r\"[^a-zA-Z0-9]\",\" \", doc)\n",
    "    doc = doc.lower()\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    \n",
    "    word_to_index = {}\n",
    "    bow = []\n",
    "    \n",
    "    for word in tokenized_doc:\n",
    "        if word not in word_to_index.keys():\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            bow.insert(-1, 1)\n",
    "        else:\n",
    "            bow[word_to_index[word]] += 1\n",
    "    return word_to_index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2da3c0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text의 word_to_index: {'family': 0, 'is': 1, 'not': 2, 'an': 3, 'important': 4, 'thing': 5, 'it': 6, 's': 7, 'everything': 8}\n",
      "text의 bow: [1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "word_to_index, text_bow = en_bag_of_words(text)\n",
    "\n",
    "print(\"text의 word_to_index:\", word_to_index)\n",
    "print(\"text의 bow:\", text_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b086c983",
   "metadata": {},
   "source": [
    "만약, word_to_index가 이미 존재하고 있고 이를 다른 문장에 적용한다면\n",
    "\n",
    "    - 단어가 word_to_index에 전부 존재한다는 전제하에 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a78c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"Family is not an important thing.\"\n",
    "text2 = \"It's everything.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38bb3166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_bag_of_words(doc, word_to_index):\n",
    "    doc = re.sub(r\"[^a-zA-Z0-9]\",\" \", doc)\n",
    "    doc = doc.lower()\n",
    "    tokenized_doc = word_tokenize(doc)\n",
    "    \n",
    "    bow = [0 for i in range(len(word_to_index))]\n",
    "    \n",
    "    for word in tokenized_doc:\n",
    "        bow[word_to_index[word]] += 1\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3dd37ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1의 BoW: [1, 1, 1, 1, 1, 1, 0, 0, 0]\n",
      "text2의 BoW: [0, 0, 0, 0, 0, 0, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"text1의 BoW:\", en_bag_of_words(text1, word_to_index))\n",
    "print(\"text2의 BoW:\", en_bag_of_words(text2, word_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99853f3",
   "metadata": {},
   "source": [
    "#### 영어 DTM 구현 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ed8303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Family is not an important thing.\",\n",
    "    \"It's everything.\",\n",
    "    \"Family is not an important thing. It's everything\",\n",
    "    \"everything is Love\",\n",
    "    \"Love is everything.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cba990a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab(text):\n",
    "    doc = []\n",
    "    \n",
    "    for sent in text:\n",
    "        sent = re.sub(r\"[^a-zA-Z0-9]\",\" \", sent)\n",
    "        sent = sent.lower()\n",
    "        token_sent = word_tokenize(sent)\n",
    "        for word in token_sent:\n",
    "            if word not in doc:\n",
    "                doc.append(word)\n",
    "    doc.sort()\n",
    "    vocab = {}\n",
    "    for idx, word in enumerate(doc):\n",
    "        vocab[word] = idx\n",
    "        \n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b3a1dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 0,\n",
       " 'everything': 1,\n",
       " 'family': 2,\n",
       " 'important': 3,\n",
       " 'is': 4,\n",
       " 'it': 5,\n",
       " 'love': 6,\n",
       " 'not': 7,\n",
       " 's': 8,\n",
       " 'thing': 9}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = make_vocab(text)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4388d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DTM(text, vocab):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\",\" \", text)\n",
    "    text = text.lower().split()\n",
    "    dtm = [0 for i in range(len(vocab))]\n",
    "    for word in text:\n",
    "        if word in vocab.keys():\n",
    "            dtm[vocab[word]] += 1\n",
    "    return dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4391285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5ac0bf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>everything</th>\n",
       "      <th>family</th>\n",
       "      <th>important</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>love</th>\n",
       "      <th>not</th>\n",
       "      <th>s</th>\n",
       "      <th>thing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   an  everything  family  important  is  it  love  not  s  thing\n",
       "0   1           0       1          1   1   0     0    1  0      1\n",
       "1   0           1       0          0   0   1     0    0  1      0\n",
       "2   1           1       1          1   1   1     0    1  1      1\n",
       "3   0           1       0          0   1   0     1    0  0      0\n",
       "4   0           1       0          0   1   0     1    0  0      0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\n",
    "    \"Family is not an important thing.\",\n",
    "    \"It's everything.\",\n",
    "    \"Family is not an important thing. It's everything\",\n",
    "    \"everything is Love\",\n",
    "    \"Love is everything.\"\n",
    "]\n",
    "\n",
    "col = vocab.keys()\n",
    "text_df = []\n",
    "\n",
    "for idx, sent in enumerate(text):\n",
    "    text_df.append(DTM(sent, vocab))\n",
    "\n",
    "dtm_ = pd.DataFrame(text_df, columns=col)\n",
    "dtm_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909a390",
   "metadata": {},
   "source": [
    "#### TF-IDF 제작 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eeb4f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c17aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(dtm, vocab):\n",
    "    df = [0 for i in range(len(vocab))]\n",
    "    for i in range(len(dtm)):\n",
    "        for j in range(len(vocab)):\n",
    "            if dtm[i][j] != 0:\n",
    "                df[j] += 1\n",
    "    return df\n",
    "\n",
    "def idf(df, dtm, vocab):\n",
    "    idf = [0 for i in range(len(vocab))]\n",
    "    for i in range(len(vocab)):\n",
    "        idf[i] = np.log(len(dtm)/(1+df[i]))\n",
    "    return idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe802036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TF_IDF(dtm, vocab):\n",
    "    df_ = df(dtm, vocab)\n",
    "    idf_ = idf(df_, dtm, vocab)\n",
    "    print(\"df 값:\", df_)\n",
    "    for i in range(len(dtm)):\n",
    "        for j in range(len(vocab)):\n",
    "            if dtm[i][j] != 0:\n",
    "                dtm[i][j] = idf_[j]\n",
    "    return dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "efec2f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df 값: [2, 4, 2, 2, 4, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>everything</th>\n",
       "      <th>family</th>\n",
       "      <th>important</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>love</th>\n",
       "      <th>not</th>\n",
       "      <th>s</th>\n",
       "      <th>thing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.510826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         an  everything    family  important   is        it      love  \\\n",
       "0  0.510826         0.0  0.510826   0.510826  0.0  0.000000  0.000000   \n",
       "1  0.000000         0.0  0.000000   0.000000  0.0  0.510826  0.000000   \n",
       "2  0.510826         0.0  0.510826   0.510826  0.0  0.510826  0.000000   \n",
       "3  0.000000         0.0  0.000000   0.000000  0.0  0.000000  0.510826   \n",
       "4  0.000000         0.0  0.000000   0.000000  0.0  0.000000  0.510826   \n",
       "\n",
       "        not         s     thing  \n",
       "0  0.510826  0.000000  0.510826  \n",
       "1  0.000000  0.510826  0.000000  \n",
       "2  0.510826  0.510826  0.510826  \n",
       "3  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_ = TF_IDF(text_df,vocab)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tfidf_, columns=col)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc17bdf7",
   "metadata": {},
   "source": [
    "df 값이 존재하는데 idf가 0인 것을 확인 할 수 있다.\n",
    "- idf의 (분모 + 1)와 분자항이 일치하면 로그 진수 값이 1이 되면서 idf 값이 0이 된다. \n",
    "- everything과 is는 log의 진수값이 1이 되면서 0이 된것을 확인 할 수 있다. \n",
    "- idf값이 0이 된다면 보기와 같이 가중치의 역할을 수행하지 못한다. \n",
    "\n",
    "- 사이킷런과 같은 라이브러리의 경우, 위의 문제가 발생하지 않도록 식을 조정한다고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b0399",
   "metadata": {},
   "source": [
    "#### tf-idf를 cosine similarity로 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd9ac18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(A, B):\n",
    "    return np.dot(A, B) / np.linalg.norm(A) + np.linalg.norm(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36080c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Family is not an important thing.\",\n",
    "    \"It's everything.\",\n",
    "    \"Family is not an important thing. It's everything\",\n",
    "    \"everything is Love\",\n",
    "    \"Love is everything.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c0f3af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family is not an important thing.과 It's everything. 사이의 유사도: 0.72241652513756\n",
      "Family is not an important thing.과 Family is not an important thing. It's everything 사이의 유사도: 2.493758383193745\n",
      "Family is not an important thing.과 everything is Love 사이의 유사도: 0.5108256237659907\n",
      "Family is not an important thing.과 Love is everything. 사이의 유사도: 0.5108256237659907\n"
     ]
    }
   ],
   "source": [
    "print(\"Family is not an important thing.과 It's everything. 사이의 유사도:\", cosine_sim(tfidf_[0], tfidf_[1]))\n",
    "print(\"Family is not an important thing.과 Family is not an important thing. It's everything 사이의 유사도:\", cosine_sim(tfidf_[0], tfidf_[2]))\n",
    "print(\"Family is not an important thing.과 everything is Love 사이의 유사도:\", cosine_sim(tfidf_[0], tfidf_[3]))\n",
    "print(\"Family is not an important thing.과 Love is everything. 사이의 유사도:\", cosine_sim(tfidf_[0], tfidf_[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad211ca4",
   "metadata": {},
   "source": [
    "Family is not an important thing.와 다른 문서의 유사도 중 Family is not an important thing. It's everything의 유사도가 가장 높은 것을 확인할 수 있다. 이는 같은 문장이 있는 문서를 잘 찾은듯 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d311fef",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
